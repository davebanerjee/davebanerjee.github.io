---
layout:               post
title:                "College EA Groups Are Failing"
subtitle:             insert description here
date:                 2023-07-18 00:51:56 -0500
last_modified_at:     
readable_date:        
permalink:            /blog/ea-experience
image:                /assets/ea-criticism/students-walking-into-campus.jpg
image_desc:           students walking into campus
read_time:            
featured:             true
comments:             true
---

I recently resigned as Columbia EA President and have stepped away from the EA community. This post aims to explain my EA experience and some reasons why I am leaving EA. I will discuss poor epistemic norms in uni groups, why retreats can be manipulative, and why paying uni group organizers may be harmful. Most of my views on uni group dynamics are informed by my experience with Columbia EA. My knowledge of other uni groups comes from conversations with other organizers, but I will never have an accurate picture of the uni group ecosystem.

1. 
{:toc}

# My EA Experience

During my freshman year, I heard about a club called Columbia Effective Altruism. Rumor on the street told me it was a cult, but I was intrigued. Every week, my friend would return from the fellowship and share what he learned. I was fascinated. Once spring rolled around, I applied for the spring Arete Fellowship.

After enrolling in the fellowship, I quickly fell in love with Effective Altruism. Everything about EA seemed just rightâ€”it was the perfect club for me. EAs were talking about the biggest and most important ideas of our time. The EA community was everything I hoped college to be. I felt like I found my people. I found people who actually cared about improving the world. I found people who strived to tear down the sellout culture at Columbia.

After completing the Arete Fellowship, I reached out to the organizers asking how I could get more involved. They told me about EAG SF and a longtermist community builder retreat. Excited, I applied to both and was accepted. After just three months after getting involved with EA, I was flown out to SF to a fancy conference and a seemingly exclusive retreat.

EAG SF was a lovely experience. I met many people who inspired me to be more ambitious. My love for EA was further cementing. I felt psychologically safe and welcomed. After about thirty one-on-ones, the conference was over, and I was on my way to an ~exclusive~ retreat.

I like to think I'm a very social person who can navigate social situations elegeantly, but at this retreat, I felt totally lost. All these people around me were talking about so many weird ideas I knew nothing about. When I'd hear these ideas, I didn't really know what to do besides nod my head and occasionally say "that makes sense." After each one-on-one, I knew that I shouldn't update my beliefs too much, but after hearing almost every person talk about how AI Safety is the most important cause area, I couldn't help but be convinced. By the end of the retreat, I went home a self-proclaimed longtermist who prioritized AI safety. 

It took several months to sober up. After rereading some important EA criticisms (Bad Omens, Doing EA Better, etc.), I realized I got duped. My poor epistemics led me astray, but weirdly enough, my poor epistemics gained me some social points in EA circles. While at the retreat and at EA events afterwards, I was socially rewarded for telling people that I was a longtermist who cared about AI safety. Nowadays, when I tell people I'm not necesarilly a longtermist and don't prioritize AI safety, the burden of proof is on me to explain why I "dissent" from EA. If you're a longtermist AI safety bro, there's no need to offer evidence to defend your view.

I became President of Columbia EA shortly after returning home from the EAG SF and the retreat, and I'm afraid did some pretty bad community building. Here are two mistakes I made:

1. In the final week of the Arete Fellowship (I was facilitating), I asked the participants what they thought the most pressing problem was. One said climate change, two said global health, and two said AI safety. Neither of the people who said AI safety had any background in AI. If after Arete, someone (without background in AI) decides that AI safety is the most important issue, then something has gone wrong. I suspect they are prioritizing AI safety on shaky epistemic ground. Unfortunately, rather than flagging this as epistemically shaky, I dedicated my time and resources to push them to apply to EAG(x)'s, GCP workshops, and our other advanced fellowships. I did not follow up with anyone else in the cohort.
2. I hosted a retreat with students from Columbia, Cornell, NYU, and UPenn. All participants were new EAs (either still completing Arete or just finished Arete). I think I felt pressure to host a retreat because "that's what all good community builders do." The social dynamics at this retreat were pretty solid (in my opinion), but afterwards I felt discontent. I had not convinced any of the participants to take EA seriously, and I felt like I had failed. Even though I knew that convincing people of EA wasn't necessarily the goal, I still implicitly aimed for that goal.

I served as president for a year and have since stepped down and dissociated myself from EA. I don't know if/when I will rejoin the community, but I was asked to share my concerns about EA, particularly uni groups, so here it is!

# Epistemic Problems in Undergraduate EA Communities

*Every highly engaged EA I know has converged on AI safety as the most pressing problem*. Whether or not they have a background in AI, they have converged on AI Safety. The notable exceptions are those who were already deeply committed to animal welfare or those who have a strong background in biology. The pre-EA animal welfare folks pursue careers in animal welfare, and the pre-EA biology folks pursue careers in biosecurity. To me, these notable exceptions have probably not performed rigorous cause prioritization. For those who converge on AI Safety, I also think it's unlikely that they have performed rigorous cause prioritization. I don't think this is *that bad* because cause prioritization is super hard, especially if your cause prioritization leads you to work on a cause you have no prior experience in. I am scared of a community that emphasizes the importance of cause prioritization yet no one actually cause prioritizes. Perhaps, people are okay with deferring their cause prio to EA orgs like 80k, but I don't think many people would have the balls to openly admit that their cause prio is a result of deferral. We often think of cause prio as key to the EA project and to admit to deferring on one's cause prio is to essentially reject the project of Effective Altruism.

## My Best Guess on Why AI Safety Grips Undergraduate Students

I see college EA groups as factories for churning out people who care about existential risk reduction. Here's how I see the Intro (Arete) Fellowship:

1. Woah! There's an immense opportunity to do good! You can use your money and your time to change the world!
2. Wow! Some charities are way better than others!
3. Empathy! That's nice. Let's empathize with animals!
4. Doom! The world might end?! You *should* take this more seriously than everything we've talked about before in this fellowship
5. Longtermism! You *should* care about future beings. Oh, you think that's a weird thing to say... well, why don't you <a href="https://www.lesswrong.com/tag/taking-ideas-seriously" target="_blank">*take ideas more seriously*</a>?!
6. AI is going to kill us all! You should be working on this. 80k told me to tell you that you should work on this.
7. This week we'll be discussing WHAT ~YOU~ THINK! However, if you say anything against EA, I (your facilitator) will lecture for a few minutes defending EA.
8. Time to actually do stuff! Go to EAG! Go to a retreat! Go to the Bay!

I'm obviously exaggerating what the EA fellowship experience is like, but I think this is pretty close to describing the dynamics of EA fellowships. And once the fellowship is over, the people who stick around are those who were sold on the ideas espoused in weeks 4, 5, and 6 (existential risks, longtermism, and AI) either because all their facilitators were passionate about those topics, they were tech bros, or they were inclined to those ideas due to some non-cognitive mechanism (emotion). The folks who were intrigued by weeks 1, 2, and 3 (animal welfare, global health, and cost-effectiveness) but dismissed longtermism, x-risks, or AI safety will quickly realize there is no place for them in EA. Over time, the EA group continues to select for people with those values, and before you know it your EA group is now a factory that churns out x-risk reducers, longtermists, and AI doomers. I am especially fearful that almost every person who becomes highly engaged due to their college group is going to have world views that are strikingly similar to those who compiled the EA handbook (intro fellowship syllabus) and AGISF.

It may be that AI Safety is in fact the most important problem of our time, but there is an epistemic problem in EA groups that cannot be ignored. Some *acclaimed* EA groups might be excellent at churning out excellent AI Safety researchers, but should we really acclaim groups that churn out smart people. I would prefer to have a smaller, epistemically healthy group that embarks on the project of Effective Altruism.

### Caveats

I suspect that I overestimate how much facilitators influence fellows' thinking. I think that the people who become highly engaged don't become highly engaged becuase their facilitator was very persuasive (persuasiveness is a small part); rather, people become highly engaged because they are tech bros, are sympathetic towards techno-utopianism, have been raised in some culture that makes them more primed to accept x-risk arguments, or they have never faced dramatic oppression or personal obstacle (such people would probably be more focused on neartermist interventions).

Unfortunately, I don't have an alternative to the classic fellowship structure. 

## How Retreats Foster an Epistemically Unhealthy Culture

In this section, I will argue that retreats result in people taking ideas seriously when they perhaps shouldn't. Retreats make people more susceptible to buy into weird ideas. Those weird ideas may in fact be true, but the process is not truth-seeking.

### Against Taking Ideas Seriously

According to LessWrong, "Taking Ideas Seriously is the skill/habit of noticing when a new idea should have major ramifications." I think taking ideas seriously can be a useful skill, but I'm hesitant when people encourage new EAs to take ideas seriously.

Scott Alexander <a href="https://slatestarcodex.com/2019/06/03/repost-epistemic-learned-helplessness/" target="_blank">warns against taking ideas seriously</a>:

> for 99% of people, 99% of the time, taking ideas seriously is the wrong strategy. Or, at the very least, it should be the last skill you learn, after youâ€™ve learned every other skill that allows you to know which ideas are or are not correct. The people I know who are best at taking ideas seriously are those who are smartest and most rational. I think people are working off a model where these co-occur because you need to be very clever to resist your natural and detrimental tendency not to take ideas seriously. But I think they might instead co-occur because you have to be really smart in order for taking ideas seriously not to be immediately disastrous. You have to be really smart not to have been talked into enough terrible arguments.

### Why Do People Take Ideas Seriously in Retreats?

<a href="https://forum.effectivealtruism.org/posts/oYbQcobBA9dZ8qKSK/university-groups-should-do-more-retreats" target="_blank">Retreats are often deemed the most effective university community building strategy</a>. Retreats heavily increase people's engagement with EA. People cite retreats as being key to their onramp to EA and taking ideas like AI Safety, x-risks, and longtermism seriously. I think retreats make people take ideas more seriously because retreats disable people's epistemic immune system.

1. Retreats are a foreign place. You are uncomfortable and less likely to put "yourself out there." Disagreeing with the organizers, for example, puts "yourself out there." Thus, you are unlikely to dissent with the organizers or the speakers. 
2. You will be talking to people from a wide range of involvement with EA. If you are a newbie at an EA retreat and most of the other people are experienced EAs, you will likely be easily convinced of their strongly held beliefs because you've never heard of these ideas, and they just have a lot more experience thinking about these things. You may begin to trust them but then you realize that they are part of a longer chain. This person likely heard a similar take from someone confident when they were a younger EA. Now that belief has expanded and rooted itself in multiple people. GG bro
3. When people say things confidently about topics you don't know about, there's not much to do. For five days, you are bombarded with arguments for AI safety, and what the hell can you do in response. Sit in your room and try to read arguments and counterarguments so you can be prepared to talk about these issues the next day? ABSOLUTELY NOT. The point of this retreat is to talk to people about big ideas that will change the world. You're not there to do the due diligence of thinking through new, foreign ideas. At this retreat, you are heavily encouraged to take advantage of all the networking opportunities. With no opportunity to do your due diligence to read into what people are confidently talking about, you are forced to implicitly trust your fellow retreat participants. Suddenly, you will have unusually high credences in everything that people have been talking about. Even if you decide to do your due diligence after the retreat, you will be fighting an uphill battle against your unusually high prior on those "out there" takes from those *really smart* people at the retreat.

### Other Retreat Issues

1. Social dynamics are super weird. It can feel very alienating if you don't know anyone at the retreat while everyone else seems to know each other.
    1. A solution may be to host lots of speed friending in the beginning of the retreat
    2. People should get to choose who they want to speed friend with based on people's bio
    3. Random speed friending does not work very well in my opinion. People need something to talk about that they both have a stake in.
2. Lack of psychological safety
    1. Organizers need to remind participants to stop talking behind each other's backs. This is such childish behavior...
    2. I think it's fine for conversations at retreats to be focused on sharing ideas and generating impact, but people should interact with each other more humanly. It shouldn't feel like the only point of the conversation is impact. Relationships shouldn't be centered around impact. No one should ever feel that they will jeopardize a relationship if they stop being appearing to be impactful.
3. Not related to retreats but kinda similar: sending people to the Bay Area is *weird*. Why do people suddenly start to take longtermist, x-risk, AI safety ideas more seriously when they move to the Bay? I suspect moving to the Bay Area has similar effects as going to retreats.

# University Group Organizer Funding

University Group Organizers should not be paid so much. I was paid an outrageous amount of money to lead my university's EA group. My hourly rate was more than $40/hr. **I will not apply for organizer funding again even if I do community build in the future.**

## Why Paying Organizers Is Bad

1. If my campus newspaper found out I was being paid this much, I am sure an EA take-down article would be published instantly. I can see the headline already "Defective Altruism: How Self-Righteous Tech Bros Think They're Saving the World by Paying Themselves."
2. I highly doubt paying uni group organizers this much is increasing their counterfactual impact. I don't think organizers are spending much more time because of this payment. Most EA organizers are from wealthy backgrounds, so the money is not clearing many bottlenecks (need-based funding would be great. See potential fixes section).
    1. Paying organizers probably doesn't make them take their roles more seriously. I'd be curious to read the results of the uni group organizer funding exit survey.
3. This money is getting taxed and lost to the notably cost-ineffective government (maybe not that relevant)
4. Being paid to run a college club is *SO WEIRD*. Everyone volunteers to run college clubs. I don't buy the argument that Open Phil is compensating organizers because they are against unpaid labor. I think Open Phil is paying organizers because of impact consideration, not because of some leftist ideology.

## Potential Fixes

1. Turn the University Group Organizer Fellowship into a need-based fellowhship. This is likely to eliminate financial bottlenecks in people's lives and accelerate their path to impact, while not wasting money on those whom do not face financial bottlenecks.
2. If the the University Group Organizer Fellowship exit survey indicates that funding was somewhat helpful in increasing people's commitment to quality community building, then reduce funding to $15/hr. If the results indicate that funding had little to no impact, abandon funding (not worth the reputational risks and weirdness). I honestly don't see a scenario where the results of the survey indicate that the funding was exceptionally impactful.