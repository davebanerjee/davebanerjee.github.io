---
layout:               post
title:                "The Convergence on AI Safety"
subtitle:             insert description here
date:                 2023-07-18 00:51:56 -0500
last_modified_at:     2023-07-18 00:51:56 -0500
permalink:            /blog/ea-experience
image:                
featured:             true
comments:             true
---



1. 
{:toc}

# Undergraduate Communities

*Every highly engaged EA I know has converged on AI Safety*. Whether or not they have a background in AI, they have converged on AI Safety. The notable exceptions are those who were already deeply committed to animal welfare or those who have a strong background in biology. The pre-EA animal welfare folks pursue careers in animal welfare, and the pre-EA biology folks pursue careers in biosecurity. To me, these notable exceptions have probably not performed rigorous cause prioritization. For those who converge on AI Safety, I also think it's unlikely that they have performed rigorous cause prioritization. I don't think this is *that bad* because cause prioritization is super hard, especially if your cause prioritization leads you to work on a cause you have no prior experience in. I am scared of a community that emphasizes the importance of cause prioritization yet no one actually cause prioritizes. Perhaps, people are okay with deferring their cause prio to EA orgs like 80k, but I don't think many people would have the balls to openly admit that their cause prio is a result of deferral. We often think of cause prio as key to the EA project and to admit to deferring on one's cause prio is to essentially reject the project of Effective Altruism.

# Why Does AI Safety Grip Undergraduate Students

I see college EA groups as factories for churning out people who care about existential risk reduction. Here's how I see the Intro (Arete) Fellowship:

1. Woah! There's an immense opportunity to do good! You can use your money and your time to change the world!
2. Wow! Some charities are way better than others!
3. Empathy! That's nice. Let's empathize with animals!
4. Doom! The world might end?! You *should* take this more seriously than everything we've talked about before in this fellowship
5. Longtermism! You *should* care about future beings. Oh, you think that's a weird thing to say... well, why don't you *take ideas more seriously*?!
6. AI is going to kill us all! You should be working on this. 80k told me to tell you that you should work on this.
7. This week we'll be discussing WHAT ~YOU~ THINK! However, if you say anything against EA, I (your facilitator) will lecture for a few minutes defending EA.
8. Time to actually do stuff! Go to EAG! Go to a retreat! Go to the Bay!

I'm obviously exaggerating what the EA fellowship experience is like, but I think this is pretty close to describing the dynamics of EA fellowships. And once the fellowship is over, the people who stick around are those who were sold on the ideas espoused in weeks 4, 5, and 6 (existential risks, longtermism, and AI) either because all their facilitators were passionate about those topics, they were tech bros, or they were inclined to those ideas due to some non-cognitive mechanism (emotion). Over time, the EA group continues to select for people with those values, and before you know it your EA group is now a factory that churns out existential risk reducers, longtermists, and AI doomers. 

It may be that AI Safety is in fact the most important problem of our time, but there is an epistemic problem in EA groups that cannot be ignored. Some *acclaimed* EA groups might be excellent at churning out excellent AI Safety researchers, but should we really acclaim groups that churn out smart people. I would prefer to have a smaller, epistemically healthy group that embarks on the project of Effective Altruism.

# Retreats

Retreats can be soooo weird. 